{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fe25e59",
   "metadata": {
    "papermill": {
     "duration": 0.010058,
     "end_time": "2023-08-31T11:20:04.625088",
     "exception": false,
     "start_time": "2023-08-31T11:20:04.615030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Binary Sentiment Analysis on IMDB Reviews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a3762",
   "metadata": {
    "papermill": {
     "duration": 0.009035,
     "end_time": "2023-08-31T11:20:04.643681",
     "exception": false,
     "start_time": "2023-08-31T11:20:04.634646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Libraries & Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e283c0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:20:04.664980Z",
     "iopub.status.busy": "2023-08-31T11:20:04.664021Z",
     "iopub.status.idle": "2023-08-31T11:20:14.756856Z",
     "shell.execute_reply": "2023-08-31T11:20:14.755613Z"
    },
    "papermill": {
     "duration": 10.106952,
     "end_time": "2023-08-31T11:20:14.759867",
     "exception": false,
     "start_time": "2023-08-31T11:20:04.652915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Env :  3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0]\n",
      "TensorFlow :  2.12.0\n"
     ]
    }
   ],
   "source": [
    "# Manage Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Libraris & Modules\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import string\n",
    "import re as regex\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow Log Level\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# TensorFlow Libraries\n",
    "import tensorflow as tf\n",
    "\n",
    "# Environment/Versions\n",
    "print(\"Python Env : \", sys.version)\n",
    "print(\"TensorFlow : \", tf.__version__)\n",
    "\n",
    "# Random Generator Seed\n",
    "random_seed = 47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e9fcbc",
   "metadata": {
    "papermill": {
     "duration": 0.009166,
     "end_time": "2023-08-31T11:20:14.778669",
     "exception": false,
     "start_time": "2023-08-31T11:20:14.769503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Downloading & Exploring Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738c525d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:20:14.800562Z",
     "iopub.status.busy": "2023-08-31T11:20:14.799804Z",
     "iopub.status.idle": "2023-08-31T11:20:49.653270Z",
     "shell.execute_reply": "2023-08-31T11:20:49.652233Z"
    },
    "papermill": {
     "duration": 34.867974,
     "end_time": "2023-08-31T11:20:49.656125",
     "exception": false,
     "start_time": "2023-08-31T11:20:14.788151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84125825/84125825 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Dataset Download Path\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# Retrieve File (and untar)\n",
    "dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1\", \n",
    "                                  origin=url, \n",
    "                                  untar=True, \n",
    "                                  cache_dir=\".\", \n",
    "                                  cache_subdir=\"\")\n",
    "\n",
    "# Dataset Directory\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722d8d4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:20:49.686307Z",
     "iopub.status.busy": "2023-08-31T11:20:49.685536Z",
     "iopub.status.idle": "2023-08-31T11:20:49.694221Z",
     "shell.execute_reply": "2023-08-31T11:20:49.692966Z"
    },
    "papermill": {
     "duration": 0.026108,
     "end_time": "2023-08-31T11:20:49.696735",
     "exception": false,
     "start_time": "2023-08-31T11:20:49.670627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'imdbEr.txt', 'README', 'test', 'imdb.vocab']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List Dataset Sub-Directories & Files\n",
    "display(os.listdir(dataset_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa5763ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:20:49.725421Z",
     "iopub.status.busy": "2023-08-31T11:20:49.724522Z",
     "iopub.status.idle": "2023-08-31T11:20:49.731705Z",
     "shell.execute_reply": "2023-08-31T11:20:49.730556Z"
    },
    "papermill": {
     "duration": 0.02434,
     "end_time": "2023-08-31T11:20:49.734449",
     "exception": false,
     "start_time": "2023-08-31T11:20:49.710109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset v1.0\n",
      "\n",
      "Overview\n",
      "\n",
      "This dataset contains movie reviews along with their associated binary\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\n",
      "sentiment classification. This document outlines how the dataset was\n",
      "gathered, and how to use the files provided. \n",
      "\n",
      "Dataset \n",
      "\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
      "documents for unsupervised learning. \n",
      "\n",
      "In the entire collection, no more than 30 reviews are allowed for any\n",
      "given movie because reviews for the same movie tend to have correlated\n",
      "ratings. Further, the train and test sets contain a disjoint set of\n",
      "movies, so no significant performance is obtained by memorizing\n",
      "movie-unique terms and their associated with observed labels.  In the\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
      "more neutral ratings are not included in the train/test sets. In the\n",
      "unsupervised set, reviews of any rating are included and there are an\n",
      "even number of reviews > 5 and <= 5.\n",
      "\n",
      "Files\n",
      "\n",
      "There are two top-level directories [train/, test/] corresponding to\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\n",
      "the reviews with binary labels positive and negative. Within these\n",
      "directories, reviews are stored in text files named following the\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
      "the star rating for that review on a 1-10 scale. For example, the file\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
      "omitted for this portion of the dataset.\n",
      "\n",
      "We also include the IMDb URLs for each review in a separate\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
      "are unable to link directly to the review, but only to the movie's\n",
      "review page.\n",
      "\n",
      "In addition to the review text files, we include already-tokenized bag\n",
      "of words (BoW) features that were used in our experiments. These \n",
      "are stored in .feat files in the train/test directories. Each .feat\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
      "data.  The feature indices in these files start from 0, and the text\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
      "(the) appears 7 times in that review.\n",
      "\n",
      "LIBSVM page for details on .feat file format:\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
      "\n",
      "We also include [imdbEr.txt] which contains the expected rating for\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
      "rating is a good way to get a sense for the average polarity of a word\n",
      "in the dataset.\n",
      "\n",
      "Citing the dataset\n",
      "\n",
      "When using this dataset please cite our ACL 2011 paper which\n",
      "introduces it. This paper also contains classification results which\n",
      "you may want to compare against.\n",
      "\n",
      "\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "  month     = {June},\n",
      "  year      = {2011},\n",
      "  address   = {Portland, Oregon, USA},\n",
      "  publisher = {Association for Computational Linguistics},\n",
      "  pages     = {142--150},\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "}\n",
      "\n",
      "References\n",
      "\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
      "636-659.\n",
      "\n",
      "Contact\n",
      "\n",
      "For questions/comments/corrections please contact Andrew Maas\n",
      "amaas@cs.stanford.edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review Readme File\n",
    "with open(os.path.join(dataset_dir, 'README'), 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a5e50b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:20:49.763680Z",
     "iopub.status.busy": "2023-08-31T11:20:49.762954Z",
     "iopub.status.idle": "2023-08-31T11:20:49.773226Z",
     "shell.execute_reply": "2023-08-31T11:20:49.772152Z"
    },
    "papermill": {
     "duration": 0.027788,
     "end_time": "2023-08-31T11:20:49.775827",
     "exception": false,
     "start_time": "2023-08-31T11:20:49.748039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['urls_neg.txt',\n",
       " 'unsup',\n",
       " 'pos',\n",
       " 'urls_pos.txt',\n",
       " 'neg',\n",
       " 'labeledBow.feat',\n",
       " 'urls_unsup.txt',\n",
       " 'unsupBow.feat']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['urls_neg.txt', 'pos', 'urls_pos.txt', 'neg', 'labeledBow.feat']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train & Test Directories\n",
    "display(os.listdir(f\"{dataset_dir}/train\"))\n",
    "display(os.listdir(f\"{dataset_dir}/test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e59161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:20:49.808072Z",
     "iopub.status.busy": "2023-08-31T11:20:49.806785Z",
     "iopub.status.idle": "2023-08-31T11:20:49.816159Z",
     "shell.execute_reply": "2023-08-31T11:20:49.815264Z"
    },
    "papermill": {
     "duration": 0.029326,
     "end_time": "2023-08-31T11:20:49.818828",
     "exception": false,
     "start_time": "2023-08-31T11:20:49.789502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['urls_neg.txt',\n",
       " 'unsup',\n",
       " 'pos',\n",
       " 'urls_pos.txt',\n",
       " 'neg',\n",
       " 'labeledBow.feat',\n",
       " 'urls_unsup.txt',\n",
       " 'unsupBow.feat']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Data Directory\n",
    "train_dir = os.path.join(dataset_dir, \"train\")\n",
    "train_dir_items = os.listdir(train_dir)\n",
    "train_dir_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bca2a75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:20:49.848606Z",
     "iopub.status.busy": "2023-08-31T11:20:49.847793Z",
     "iopub.status.idle": "2023-08-31T11:20:49.854696Z",
     "shell.execute_reply": "2023-08-31T11:20:49.853859Z"
    },
    "papermill": {
     "duration": 0.024371,
     "end_time": "2023-08-31T11:20:49.856919",
     "exception": false,
     "start_time": "2023-08-31T11:20:49.832548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['urls_neg.txt', 'pos', 'urls_pos.txt', 'neg', 'labeledBow.feat']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing Data Directory\n",
    "test_dir = os.path.join(dataset_dir, \"test\")\n",
    "os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b59be63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:20:49.887212Z",
     "iopub.status.busy": "2023-08-31T11:20:49.886212Z",
     "iopub.status.idle": "2023-08-31T11:20:49.901535Z",
     "shell.execute_reply": "2023-08-31T11:20:49.900607Z"
    },
    "papermill": {
     "duration": 0.033355,
     "end_time": "2023-08-31T11:20:49.904325",
     "exception": false,
     "start_time": "2023-08-31T11:20:49.870970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File : 8303_10.txt\n",
      "\n",
      "This film quite literally has every single action movie cliche and all of them work to its advantage. Straight from Lethal Weapon Gary Busey wisecracks, shoots and chuckles through this film with such reckless abandonment it can't help but amuse and entertain. There are tanks, helicopters, machine gun battles, grenades and ice cream vans and if they aren't good enough reasons to watch this film then how about the best one...Danny Trejo. And if you don't know who Danny Trejo is then you probably won't like this film.\n"
     ]
    }
   ],
   "source": [
    "# View a sample positive review.\n",
    "train_files = os.listdir(os.path.join(train_dir, \"pos\"))\n",
    "sample_fname = train_files[2]\n",
    "print(f\"File : {sample_fname}\\n\")\n",
    "with open(os.path.join(train_dir, f\"pos/{sample_fname}\")) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7dbf9b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:20:49.934537Z",
     "iopub.status.busy": "2023-08-31T11:20:49.934145Z",
     "iopub.status.idle": "2023-08-31T11:20:50.019699Z",
     "shell.execute_reply": "2023-08-31T11:20:50.018268Z"
    },
    "papermill": {
     "duration": 0.103565,
     "end_time": "2023-08-31T11:20:50.022351",
     "exception": false,
     "start_time": "2023-08-31T11:20:49.918786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.text_dataset_from_directory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71b09db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:20:50.053533Z",
     "iopub.status.busy": "2023-08-31T11:20:50.052530Z",
     "iopub.status.idle": "2023-08-31T11:20:58.799473Z",
     "shell.execute_reply": "2023-08-31T11:20:58.798316Z"
    },
    "papermill": {
     "duration": 8.764835,
     "end_time": "2023-08-31T11:20:58.801822",
     "exception": false,
     "start_time": "2023-08-31T11:20:50.036987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_unsup', 'train', 'imdbEr.txt', 'README', 'test', 'imdb.vocab']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['urls_neg.txt', 'pos', 'urls_pos.txt', 'neg', 'labeledBow.feat']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['unsup', 'urls_unsup.txt', 'unsupBow.feat']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare Training Directory as per 'text_dataset_from_directory' Requirements.\n",
    "# -----------------------------------------------------------------------------\n",
    "# Create New Directory for Unsupervised Data\n",
    "train_unsup_dir = os.path.join(dataset_dir, \"train_unsup\")\n",
    "if not os.path.exists(train_unsup_dir):\n",
    "    os.makedirs(train_unsup_dir)\n",
    "else:\n",
    "    pass\n",
    "display(os.listdir(dataset_dir))\n",
    "# Collect Unsupervised Data Files and Directories\n",
    "file_list = [_ for _ in train_dir_items if \"unsup\" in _]\n",
    "# Source & Destination Directories\n",
    "source_dir = train_dir\n",
    "destination_dir = train_unsup_dir\n",
    "# Move Files and Directories\n",
    "for file in file_list:\n",
    "    source_file = os.path.join(source_dir, file)\n",
    "    destination_file = os.path.join(destination_dir, file)\n",
    "    # Move as per Directory/File\n",
    "    if os.path.isdir(source_file):\n",
    "        shutil.copytree(source_file, destination_file)\n",
    "        shutil.rmtree(source_file)\n",
    "    else:\n",
    "        shutil.move(source_file, destination_file)\n",
    "\n",
    "# Review Training & Unsupervised Training Data Directories\n",
    "display(os.listdir(train_dir))\n",
    "display(os.listdir(train_unsup_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa13cd4e",
   "metadata": {
    "papermill": {
     "duration": 0.014757,
     "end_time": "2023-08-31T11:20:58.831063",
     "exception": false,
     "start_time": "2023-08-31T11:20:58.816306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training, Validation & Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef62fad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:20:58.866073Z",
     "iopub.status.busy": "2023-08-31T11:20:58.865565Z",
     "iopub.status.idle": "2023-08-31T11:21:05.694407Z",
     "shell.execute_reply": "2023-08-31T11:21:05.692975Z"
    },
    "papermill": {
     "duration": 6.850468,
     "end_time": "2023-08-31T11:21:05.697632",
     "exception": false,
     "start_time": "2023-08-31T11:20:58.847164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset :\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "\n",
      "\n",
      "Validation Dataset :\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "\n",
      "\n",
      "Testing Dataset :\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Training Dataset\n",
    "print(\"Training Dataset :\")\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(directory=train_dir, \n",
    "                                                      batch_size=8, \n",
    "                                                      validation_split=0.2, \n",
    "                                                      subset='training', \n",
    "                                                      seed=random_seed)\n",
    "\n",
    "# Validation Dataset\n",
    "print(\"\\n\")\n",
    "print(\"Validation Dataset :\")\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(directory=train_dir, \n",
    "                                                    batch_size=8, \n",
    "                                                    validation_split=0.2, \n",
    "                                                    subset='validation', \n",
    "                                                    seed=random_seed)\n",
    "\n",
    "# Testing Dataset\n",
    "print(\"\\n\")\n",
    "print(\"Testing Dataset :\")\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(directory=test_dir, \n",
    "                                                     batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32042809",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:21:05.730472Z",
     "iopub.status.busy": "2023-08-31T11:21:05.730037Z",
     "iopub.status.idle": "2023-08-31T11:21:05.903100Z",
     "shell.execute_reply": "2023-08-31T11:21:05.901706Z"
    },
    "papermill": {
     "duration": 0.192972,
     "end_time": "2023-08-31T11:21:05.906150",
     "exception": false,
     "start_time": "2023-08-31T11:21:05.713178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Element 1 (Labels)  : [8]\n",
      "Dataset Element 1 (Reviews) : [8]\n",
      "\n",
      "LABEL  : 0\n",
      "REVIEW : b\"I wouldn't say this is a *bad* movie. Unfortunately for me, I get the feeling that the more you know about fencing, the worse it gets simply due to the fact that it becomes totally unrealistic. I've been fencing since i was 14 years old, and this movie portrays it very poorly. F. Murray Abraham is good (and appears to have some fencing background), but most of the other actors--especially the students--just seem to be lost.\"\n",
      "--------------------------------------------------------------------------------\n",
      "LABEL  : 1\n",
      "REVIEW : b'Family problems abound in real life and that is what this movie is about. Love can hold the members together through out the ordeals and trials and that is what this movie is about. One man, Daddy, has the maturity and fortitude to sustain the family in the face of adversity. The kids grow up,one all be it, in the hard way, to realize that no matter how old they or a parent is, the parent still loves their children and are willing to provide them a cushion when they fall. ALL the actors portraying their characters did outstanding performances. Yes, I shed a tear along the way knowing I had had similar experiences both as a young adult and later as a parent. This true to life is one which every young adult, and parent, would do well to see, although some will not realize it until they too are parents. A must see for those who care about their families.'\n",
      "--------------------------------------------------------------------------------\n",
      "LABEL  : 1\n",
      "REVIEW : b'In \"Anne of Green Gables\" (1934), Marilla Cuthbert (Helen Westley) and Matthew Cuthbert (O.P. Heggie), middle-aged siblings who live together at Green Gables, a farm in Avonlea, on Prince Edward Island, decide to adopt a boy from distant orphanage to help on their farm. But the orphan sent to them is a precocious girl of 14 named Anne Shirley (Dawn Evelyn Paris-a veteran of Disney\\'s series of \"Alice\" shorts who later would adopt her character\\'s name). <br /><br />Anne was only 11 in Lucy Maude Montgomery\\'s source novel but the same actress could not credibly go from 11 to college age during the course of the story. The movie suffers somewhat from this concession, as many of Anne\\'s reactions and much of what she says are more entertaining coming from an eleven-year-old that from a teenager. As in the book, Anne is bright and quick, eager to please but dissatisfied with her name, her build, her freckles, and her long red hair. Being a child of imagination, however, Anne takes much joy in life, and adapts quickly to her new family and the environment of Prince Edward Island.<br /><br />In fact Anne is the original \"Teenage Drama Queen\" and the film\\'s screenwriter elected to focus on this aspect of her character. Which transformed the basic genre from mildly amusing family drama to comedy. A change that delighted audiences and that continues to frustrate reader purists. <br /><br />Since the comedy is very much in the spirit of the Montgomery\\'s story I can see no reason to take issue with the changes, but let this serve as fair warning to anyone expecting a totally faithful adaptation. The comedy element is the strength of the film as it is one of the earliest self-reflexive parodies of Hollywood conventions. The actress Anne Shirley was one of Hollywood\\'s all- time beauties and the film is in black and white. So much of the amusement is in seeing the title character\\'s endless laments about her appearance and hair color contradicted by what is appearing on the screen. Anne regularly regales her no nonsense rural companions with melodramatic lines like: \"If you refuse it will be a lifelong sorrow to me\". Perhaps the funniest moment is when she corrects the spelling of her name on the classroom blackboard. <br /><br />Tom Brown does a nice job as Anne\\'s love interest Gilbert Blythe and Sara Haden steals all the scenes in which she appears as the Cuthbert\\'s pompous neighbor. <br /><br />Then again, what do I know? I\\'m only a child.'\n",
      "--------------------------------------------------------------------------------\n",
      "LABEL  : 1\n",
      "REVIEW : b'Why did it sound like the husband kept calling her Appy ? It ruined a great episode and so I can only give it a 6. Proper grammar and pronunciation are essential to a film.<br /><br />It was very Hellraiser what with all the skin ripping though I dunno how anyone can survive without skin the skin is a vital organ to the body the biggest organ actually and without we would die. The more a horror film is true the more creepy it can be and more entertaining.<br /><br />I do admit though that the stories from the great horror directors are very disappointing and very mediocre. <br /><br />6/10 come on Yankies get your English up to par !'\n",
      "--------------------------------------------------------------------------------\n",
      "LABEL  : 0\n",
      "REVIEW : b'I have to agree with most everyone\\'s opinion that this show was poorly produced as well as written.The acting was not much more above the lower production values however I feel an actor can only rely on the material provided to them and make the best of it. In keeping with this thought I feel it is important to point out that one actor has risen and persevered well beyond this campy to tasteless production to have become a respectable and quite talented performer.I am referring to Laura Harris a Canadian born actor who has etched her way through many poorly produced shows and movies to find a place on the HBO hit \"Dead Like Me\" where she plays the role of Daisy Adair and to her credit she handles this role in an efficient manner.I remember having a typical boyhood crush on the young actress during this series where she played Ashley a soft spoken yet intelligent 7th grader.I felt as though if anyone might \"make it\" from this series it surely would be Laura Harris and true to her nature she did excel in the acting field to win the respect of many producers who now recognize her for her talent as well as unique Nordic blond allure. If you ever do have the opportunity to view this series I recommend that you have something epic to watch after wards such as the \\'Godfather\\' or perhaps \\'Beaches\\' in order to remind yourself that there is after all a great deal of true production integrity and value out there and that this series is only a low-budget reminder of what Laura Harris can simply state about her time on the show and I bet she would quote many a young actors words of defense by saying \"It\\'s a start!\"'\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Review Dataset\n",
    "for x, y in train_ds.take(1):\n",
    "    print(f\"Dataset Element 1 (Labels)  : {tf.shape(y)}\")\n",
    "    print(f\"Dataset Element 1 (Reviews) : {tf.shape(x)}\\n\")\n",
    "    for i in range(5):\n",
    "        tf.print(f\"LABEL  : {y[i]}\")\n",
    "        tf.print(f\"REVIEW : {x[i]}\")\n",
    "        tf.print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ebbb531",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:21:05.943739Z",
     "iopub.status.busy": "2023-08-31T11:21:05.942560Z",
     "iopub.status.idle": "2023-08-31T11:21:05.952245Z",
     "shell.execute_reply": "2023-08-31T11:21:05.950686Z"
    },
    "papermill": {
     "duration": 0.032042,
     "end_time": "2023-08-31T11:21:05.955526",
     "exception": false,
     "start_time": "2023-08-31T11:21:05.923484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Class Names Corresponding to Index (as Numerical Label)\n",
    "display(train_ds.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69196e0c",
   "metadata": {
    "papermill": {
     "duration": 0.017458,
     "end_time": "2023-08-31T11:21:05.990635",
     "exception": false,
     "start_time": "2023-08-31T11:21:05.973177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20202194",
   "metadata": {
    "papermill": {
     "duration": 0.018117,
     "end_time": "2023-08-31T11:21:06.026341",
     "exception": false,
     "start_time": "2023-08-31T11:21:06.008224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Custom Standardization Function\n",
    "\n",
    "Removing Punctuations & HTML Elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f82d6d5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:21:06.063127Z",
     "iopub.status.busy": "2023-08-31T11:21:06.062718Z",
     "iopub.status.idle": "2023-08-31T11:21:06.070945Z",
     "shell.execute_reply": "2023-08-31T11:21:06.069448Z"
    },
    "papermill": {
     "duration": 0.029986,
     "end_time": "2023-08-31T11:21:06.073924",
     "exception": false,
     "start_time": "2023-08-31T11:21:06.043938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function Definition : Remove HTML Tags & Punctuations\n",
    "def custom_standardize(input_data):\n",
    "    \"\"\"Remove HTML Tags & Punctuations\"\"\"\n",
    "    \n",
    "    # Convert to Lowercase\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    \n",
    "    # Remove the '<br />' Tag\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    \n",
    "    # Remove Punctuations\n",
    "    stripped_html_punc = tf.strings.regex_replace(stripped_html,\n",
    "        \"[%s]\" % regex.escape(string.punctuation), \"\"\n",
    "    )\n",
    "    \n",
    "    return stripped_html_punc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bb9d8f",
   "metadata": {
    "papermill": {
     "duration": 0.016658,
     "end_time": "2023-08-31T11:21:06.108543",
     "exception": false,
     "start_time": "2023-08-31T11:21:06.091885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization & Vectorization\n",
    "\n",
    "- Custom function for Standardization\n",
    "- Splitting strings into tokens. --> tf.keras.layers.TextVectorization()\n",
    "- Converting tokens into numbers for neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8403dbdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:21:06.147314Z",
     "iopub.status.busy": "2023-08-31T11:21:06.145000Z",
     "iopub.status.idle": "2023-08-31T11:21:06.216269Z",
     "shell.execute_reply": "2023-08-31T11:21:06.214675Z"
    },
    "papermill": {
     "duration": 0.092886,
     "end_time": "2023-08-31T11:21:06.219090",
     "exception": false,
     "start_time": "2023-08-31T11:21:06.126204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Features(Tokens) in Entire Dataset :  89600\n",
      "Maximum Output Sequence Length of Vectorization Layer :  250\n"
     ]
    }
   ],
   "source": [
    "# Calculate Maximum Features Value\n",
    "# --------------------------------\n",
    "# Collect all Word Tokens provided in Dataset\n",
    "vocab_tokens = list()\n",
    "with open(os.path.join(dataset_dir, \"imdb.vocab\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        vocab_tokens.append(line.strip())\n",
    "vocab_tokens = set(vocab_tokens)       \n",
    "\n",
    "# Calc. Max. Features rounded to next hundred\n",
    "max_features = round(len(vocab_tokens)+100, -2)\n",
    "\n",
    "# Delete Token Object\n",
    "del vocab_tokens\n",
    "\n",
    "# Display Max. Features\n",
    "print(\"Maximum Features(Tokens) in Entire Dataset : \", max_features)\n",
    "\n",
    "# Set Maximum Sequence Length\n",
    "# ---------------------------\n",
    "max_seq_len = 250\n",
    "print(\"Maximum Output Sequence Length of Vectorization Layer : \", max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fde5d09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T11:21:06.255343Z",
     "iopub.status.busy": "2023-08-31T11:21:06.254069Z",
     "iopub.status.idle": "2023-08-31T11:21:06.259760Z",
     "shell.execute_reply": "2023-08-31T11:21:06.258821Z"
    },
    "papermill": {
     "duration": 0.026285,
     "end_time": "2023-08-31T11:21:06.262405",
     "exception": false,
     "start_time": "2023-08-31T11:21:06.236120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vectorization Layer"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.6.4"
 },
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 76.70753,
   "end_time": "2023-08-31T11:21:08.309184",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-31T11:19:51.601654",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
